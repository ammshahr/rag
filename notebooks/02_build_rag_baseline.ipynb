{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88fe049",
   "metadata": {},
   "source": [
    "# ۲ – ساخت RAG ساده\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dccec279-bc36-4469-8083-2d92939dcc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<6.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.9.1-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: scikit-learn in d:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in d:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\0014163535\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.7.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Using cached sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.3/2.7 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.6/2.7 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 5.1 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading torch-2.9.1-cp313-cp313-win_amd64.whl (110.9 MB)\n",
      "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/110.9 MB 17.8 MB/s eta 0:00:07\n",
      "   ---------------------------------------- 1.3/110.9 MB 2.7 MB/s eta 0:00:41\n",
      "   ---------------------------------------- 1.3/110.9 MB 2.7 MB/s eta 0:00:41\n",
      "   ---------------------------------------- 1.3/110.9 MB 2.7 MB/s eta 0:00:41\n",
      "   ---------------------------------------- 1.3/110.9 MB 2.7 MB/s eta 0:00:41\n",
      "   ---------------------------------------- 1.3/110.9 MB 2.7 MB/s eta 0:00:41\n",
      "    --------------------------------------- 1.6/110.9 MB 1.0 MB/s eta 0:01:49\n",
      "    --------------------------------------- 2.6/110.9 MB 1.4 MB/s eta 0:01:15\n",
      "   - -------------------------------------- 5.2/110.9 MB 2.7 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 5.2/110.9 MB 2.7 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 5.2/110.9 MB 2.7 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 5.2/110.9 MB 2.7 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 5.2/110.9 MB 2.7 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 5.8/110.9 MB 1.9 MB/s eta 0:00:56\n",
      "   -- ------------------------------------- 6.3/110.9 MB 1.9 MB/s eta 0:00:55\n",
      "   -- ------------------------------------- 7.3/110.9 MB 2.1 MB/s eta 0:00:49\n",
      "   --- ------------------------------------ 8.7/110.9 MB 2.4 MB/s eta 0:00:44\n",
      "   --- ------------------------------------ 9.4/110.9 MB 2.4 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 13.1/110.9 MB 3.3 MB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 16.5/110.9 MB 3.9 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 18.9/110.9 MB 4.2 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 21.2/110.9 MB 4.5 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 21.8/110.9 MB 4.6 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 24.9/110.9 MB 4.9 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 26.2/110.9 MB 4.9 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 27.5/110.9 MB 5.0 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 28.8/110.9 MB 5.0 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 29.9/110.9 MB 5.0 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 30.7/110.9 MB 5.0 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 32.0/110.9 MB 5.0 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 33.3/110.9 MB 5.1 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 34.6/110.9 MB 5.1 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 35.9/110.9 MB 5.1 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 36.7/110.9 MB 5.1 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 37.7/110.9 MB 5.1 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 38.8/110.9 MB 5.1 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 40.1/110.9 MB 5.1 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 41.4/110.9 MB 5.1 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 43.0/110.9 MB 5.2 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 44.3/110.9 MB 5.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 45.6/110.9 MB 5.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 46.9/110.9 MB 5.3 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 48.5/110.9 MB 5.3 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 50.1/110.9 MB 5.3 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 51.4/110.9 MB 5.4 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 53.0/110.9 MB 5.4 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 54.3/110.9 MB 5.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 55.8/110.9 MB 5.5 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 57.1/110.9 MB 5.5 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 58.7/110.9 MB 5.5 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 60.0/110.9 MB 5.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 61.3/110.9 MB 5.5 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 62.9/110.9 MB 5.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 64.5/110.9 MB 5.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 65.8/110.9 MB 5.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 67.4/110.9 MB 5.6 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 68.7/110.9 MB 5.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 69.7/110.9 MB 5.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 71.0/110.9 MB 5.6 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 72.4/110.9 MB 5.6 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 73.7/110.9 MB 5.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 75.0/110.9 MB 5.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 76.3/110.9 MB 5.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 77.6/110.9 MB 5.7 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 78.9/110.9 MB 5.7 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 80.2/110.9 MB 5.7 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 81.5/110.9 MB 5.7 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 83.1/110.9 MB 5.7 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 84.4/110.9 MB 5.7 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 86.0/110.9 MB 5.8 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 87.6/110.9 MB 5.8 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 89.1/110.9 MB 5.8 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 90.4/110.9 MB 5.8 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 92.0/110.9 MB 5.8 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 93.3/110.9 MB 5.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 94.6/110.9 MB 5.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 95.7/110.9 MB 5.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 97.5/110.9 MB 5.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 98.6/110.9 MB 5.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 99.9/110.9 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 100.9/110.9 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 102.2/110.9 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 103.5/110.9 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 104.9/110.9 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 105.9/110.9 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 107.5/110.9 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  108.8/110.9 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  110.4/110.9 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  110.9/110.9 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 110.9/110.9 MB 5.8 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, torch, tokenizers, transformers, sentence-transformers\n",
      "\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "  Attempting uninstall: tokenizers\n",
      "   -------- ------------------------------- 1/5 [torch]\n",
      "   ---------------- ----------------------- 2/5 [tokenizers]\n",
      "    Found existing installation: tokenizers 0.21.4\n",
      "   ---------------- ----------------------- 2/5 [tokenizers]\n",
      "    Uninstalling tokenizers-0.21.4:\n",
      "   ---------------- ----------------------- 2/5 [tokenizers]\n",
      "      Successfully uninstalled tokenizers-0.21.4\n",
      "   ---------------- ----------------------- 2/5 [tokenizers]\n",
      "   ---------------- ----------------------- 2/5 [tokenizers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   ---------------------------------------- 5/5 [sentence-transformers]\n",
      "\n",
      "Successfully installed safetensors-0.7.0 sentence-transformers-5.2.0 tokenizers-0.22.2 torch-2.9.1 transformers-4.57.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\0014163535\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts transformers-cli.exe and transformers.exe are installed in 'C:\\Users\\0014163535\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2327d1df-c115-4f29-988a-4c95ec98f185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp313-cp313-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in d:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.13.2-cp313-cp313-win_amd64.whl (18.9 MB)\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/18.9 MB 2.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.3/18.9 MB 3.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.6/18.9 MB 4.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.5/18.9 MB 6.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 7.6/18.9 MB 7.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 10.0/18.9 MB 8.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 12.6/18.9 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 15.2/18.9 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.8/18.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.9/18.9 MB 9.4 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6043ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import faiss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aba9f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>short_link</th>\n",
       "      <th>service</th>\n",
       "      <th>subgroup</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "      <th>published_datetime</th>\n",
       "      <th>agency_name</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149198</td>\n",
       "      <td>رئیس اتحادیه ناشران و کتابفروشان تهران: ارز 42...</td>\n",
       "      <td>http://fna.ir/4b6no</td>\n",
       "      <td>فرهنگ</td>\n",
       "      <td>کتاب و ادبیات</td>\n",
       "      <td>رئیس اتحادیه ناشران و کتابفروشان تهران گفت: ار...</td>\n",
       "      <td>به گزارش خبرنگار کتاب و ادبیات خبرگزاری فارس، ...</td>\n",
       "      <td>هومان حسن پور, رئیس اتحادیه ناشران و کتاب فروش...</td>\n",
       "      <td>2021-10-16 01:20:24</td>\n",
       "      <td>FarsNews</td>\n",
       "      <td>رئیس اتحادیه ناشران و کتابفروشان تهران: ارز 42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61973</td>\n",
       "      <td>وزنه‌برداری قهرمانی آسیا| داودی مدال طلای یکضر...</td>\n",
       "      <td>http://fna.ir/17yaj</td>\n",
       "      <td>ورزشی</td>\n",
       "      <td>کشتی و وزنه برداری</td>\n",
       "      <td>علی داودی در حرکت یکضرب دسته 109+ کیلوگرم رقاب...</td>\n",
       "      <td>به گزارش خبرنگار ورزشی خبرگزاری فارس، امروز (ی...</td>\n",
       "      <td>قهرمانی آسیا, مدال طلا, وزنه‌برداری, علی داودی</td>\n",
       "      <td>2021-04-25 03:41:57</td>\n",
       "      <td>FarsNews</td>\n",
       "      <td>وزنه‌برداری قهرمانی آسیا| داودی مدال طلای یکضر...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  149198  رئیس اتحادیه ناشران و کتابفروشان تهران: ارز 42...   \n",
       "1   61973  وزنه‌برداری قهرمانی آسیا| داودی مدال طلای یکضر...   \n",
       "\n",
       "            short_link service            subgroup  \\\n",
       "0  http://fna.ir/4b6no   فرهنگ       کتاب و ادبیات   \n",
       "1  http://fna.ir/17yaj   ورزشی  کشتی و وزنه برداری   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  رئیس اتحادیه ناشران و کتابفروشان تهران گفت: ار...   \n",
       "1  علی داودی در حرکت یکضرب دسته 109+ کیلوگرم رقاب...   \n",
       "\n",
       "                                                text  \\\n",
       "0  به گزارش خبرنگار کتاب و ادبیات خبرگزاری فارس، ...   \n",
       "1  به گزارش خبرنگار ورزشی خبرگزاری فارس، امروز (ی...   \n",
       "\n",
       "                                                tags   published_datetime  \\\n",
       "0  هومان حسن پور, رئیس اتحادیه ناشران و کتاب فروش...  2021-10-16 01:20:24   \n",
       "1     قهرمانی آسیا, مدال طلا, وزنه‌برداری, علی داودی  2021-04-25 03:41:57   \n",
       "\n",
       "  agency_name                                          full_text  \n",
       "0    FarsNews  رئیس اتحادیه ناشران و کتابفروشان تهران: ارز 42...  \n",
       "1    FarsNews  وزنه‌برداری قهرمانی آسیا| داودی مدال طلای یکضر...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/news_subset.csv')\n",
    "df['full_text'] = df['title'] + \": \" + df['text']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344a703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 36272\n"
     ]
    }
   ],
   "source": [
    "def simple_chunker(text, chunk_size=800, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_len = len(text)\n",
    "    while start < text_len:\n",
    "        end = min(start + chunk_size, text_len)\n",
    "        chunks.append(text[start:end])\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "chunk_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    chunks = simple_chunker(row['full_text'])\n",
    "    for c_idx, chunk in enumerate(chunks):\n",
    "        chunk_data.append({\n",
    "            'chunk_id': f\"{idx}_{c_idx}\",\n",
    "            'doc_id': idx,\n",
    "            'text': chunk,\n",
    "            'category': row.get('category', 'unknown'),\n",
    "            'date': row.get('date', '')\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(chunk_data)\n",
    "print(f\"Total chunks: {len(df_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0589a53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a731a6b72dd54a98b1315ff13deb985c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic shape: (36272, 384)\n",
      "Lexical shape: (36272, 384)\n",
      "Combined shape: (36272, 768)\n"
     ]
    }
   ],
   "source": [
    "semantic_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "semantic_embeddings = semantic_model.encode(df_chunks['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=384)\n",
    "tfidf_matrix = tfidf.fit_transform(df_chunks['text'])\n",
    "lexical_embeddings = tfidf_matrix.toarray().astype('float32')\n",
    "\n",
    "combined_embeddings = np.hstack([semantic_embeddings, lexical_embeddings])\n",
    "combined_embeddings = combined_embeddings.astype('float32')\n",
    "\n",
    "print(f\"Semantic shape: {semantic_embeddings.shape}\")\n",
    "print(f\"Lexical shape: {lexical_embeddings.shape}\")\n",
    "print(f\"Combined shape: {combined_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a2661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_semantic = semantic_embeddings.shape[1]\n",
    "index_semantic = faiss.IndexFlatIP(d_semantic)\n",
    "index_semantic.add(semantic_embeddings)\n",
    "\n",
    "d_combined = combined_embeddings.shape[1]\n",
    "index_combined = faiss.IndexFlatIP(d_combined)\n",
    "index_combined.add(combined_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1eee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(question, top_k=3, mode='semantic'):\n",
    "    if mode == 'semantic':\n",
    "        q_emb = semantic_model.encode([question])\n",
    "        D, I = index_semantic.search(q_emb, top_k)\n",
    "    elif mode == 'combined':\n",
    "        s_emb = semantic_model.encode([question])\n",
    "        l_emb = tfidf.transform([question]).toarray().astype('float32')\n",
    "        q_emb = np.hstack([s_emb, l_emb])\n",
    "        D, I = index_combined.search(q_emb, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        if idx < len(df_chunks):\n",
    "            results.append(df_chunks.iloc[idx]['text'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b574d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(question, mode='semantic'):\n",
    "    retrieved_texts = retrieve(question, top_k=3, mode=mode)\n",
    "    context = \"\\n---\\n\".join(retrieved_texts)\n",
    "    final_answer = f\"سوال: {question}\\n\\nمستندات یافت شده:\\n{context}\\n\\n(پاسخ نهایی بر اساس ترکیب مستندات بالا تولید می‌شود)\"\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18b27ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سوال: نرخ تورم چقدر است؟\n",
      "\n",
      "مستندات یافت شده:\n",
      "ا ثبت کرده است. این افزایش نشان می‌دهد، در بحبوحه بحران زنجیره تامین در جهان، نرخ بالای تورم همچنان در این کشور تا سال آینده ادامه خواهد یافت. وزارت کار آمریکا اعلام کرد شاخص قیمت مصرف کننده در ماه گذشته میلادی 0.9 درصد افزایش یافت و در ماه سپتامبر نیز رشد 0.4 درصدی را ثبت کرده بود. در 12 ماه منتهی به اکتبر، شاخص قیمت مصرف کننده 6.2 درصد رشد داشته است. این رقم، بیشترین میزان سالانه از نوامبر 1990 به شمار می‌رود. تورم در آمریکا با فشار اقتصادی ناشی از موج تابستانی کرونا و انتشار سویه دلتا و بحران‌های عرضه دوباره شدت گرفته است. افزایش تورم در کشورها از جمله آمریکا به معضلی بزرگ تبدیل شده و نارضایتی بین مردم ایجاد کرده است، فدرال رزرو آمریکا افزایش تورم را به دلیل بالا رفتن قیمت سوخت می‌داند. بانک جهانی هم گزارش داده است: رشد قیمت کالاهای اساسی به تورم کشورها انجامیده و پیش بینی کرده که در سا\n",
      "---\n",
      "نرخ تورم مصر در حال نزدیک شدن به بالاترین سطح تاریخی: به گزارش خبرگزاری مهر به نقل از روسیا الیوم، بر اساس داده‌های مرکز آمار مصر، نرخ تورم سالانه در شهرهای مختلف این کشور در ماه مارس گذشته به ۳۲.۷ درصد افزایش یافت، در حالی که این عدد در ماه فوریه گذشته ۳۱.۹ درصد بود. با این آمار، تورم به بالاترین سطح ثبت شده خود در ژوئیه سال ۲۰۱۷ نزدیک شده است که در آن زمان به ۳۲.۹۵ درصد رسید. این افزایش تورم ناشی از افزایش ۶۲.۹ درصدی قیمت مواد غذایی و آشامیدنی در ماه مارس نسبت به مدت مشابه سال قبل بوده است. مصر از افزایش قیمت کالاها و خدمات در نتیجه افزایش قیمت دلار و کمبود آن در بازار رنج می‌برد، در حالی که این کشور برای تأمین نیازهای اولیه خود به شدت به واردات متکی است. همچنین بانک مرکزی مصر نیز اجازه کاهش ارزش لیره (جنیه) در برابر دلار را صادر کرد. این آمار و ارقام در شرایطی ثبت شده که مصر ارزش لیره ر\n",
      "---\n",
      " تاثیر سیاست‌های مالی در تورم در نظام اقتصادی استقراض دولت (بودجه) از نظام بانکی منجر به افزایش نقدینگی شده که با افزایش تقاضای کل در جامعه منجر به تورم خواهد شد. به عبارت دیگر رابطه مستقیم بین سطح عمومی قیمت‌ها و نقدینگی وجود دارد که منجر به عرضه پول شده و بالا رفتن سطح عمومی قیمت‌ها را به دنبال دارد. در صورتی که استقراض از طریق اوراق باشد، این مسئله هم منجر به بالا رفتن نرخ بهره در جامعه شده و سرمایه گذاری بخش خصوصی را کاهش می‌دهد که در حال حاضر این اتفاق در حال رخ دادن است. این نکته نباید فراموش شود که دور تسلسل افزایش سطح عمومی قیمت‌ها و نقدینگی از هر ناحیه‌ای باشد مجددا به بودجه آسیب می‌رساند، چرا که مخارج دولت افزایش می‌یابد و به دلیل کمبود منابع بازهم به کسری بودجه تبدیل خواهد شد. بنابراین بین افزایش مخارج دولت و درآمد (کسری بودجه) و سطح عمومی قیمت‌ها، یک رابطه علت و معلولی برقرار ا\n",
      "\n",
      "(پاسخ نهایی بر اساس ترکیب مستندات بالا تولید می‌شود)\n"
     ]
    }
   ],
   "source": [
    "q_sample = \"نرخ تورم چقدر است؟\"\n",
    "print(answer(q_sample, mode='combined'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa357436-2436-401c-9891-cb96161e8e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
